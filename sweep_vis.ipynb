{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f5d479-c055-4f02-8510-10d73b1151ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joel/code/sae_expected_l0/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/joel/code/sae_expected_l0/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from main import SparseAutoencoder, input_dim, hidden_dim, hook_point, model\n",
    "# from sae_vis.model_fns import AutoEncoder, AutoEncoderConfig\n",
    "# from sae_vis.data_storing_fns import SaeVisData\n",
    "# from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_lens import SAE, SAEConfig\n",
    "from sae_dashboard import sae_vis_runner\n",
    "from sae_dashboard.feature_data_generator import FeatureDataGenerator\n",
    "from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "from sae_dashboard.sae_vis_runner import SaeVisConfig, SaeVisRunner\n",
    "# from sae_dashboard.data_parsing_fns import \n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from transformer_lens import utils\n",
    "from sae_lens import ActivationsStore, SAE, run_evals\n",
    "from sae_lens.evals import EvalConfig\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ae20e9-e504-4f5b-a719-f993b712bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = 'itifyaiz'\n",
    "project_name = 'sae-expected-l0-sweep-norm'\n",
    "entity = 'PEAR-ML' \n",
    "hook_point = \"blocks.6.hook_resid_post\"\n",
    "dataset_path = \"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986e456b-18a6-4fe2-9e4e-b016b3dc8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "\n",
    "# Load in the data (it's a Dataset object)\n",
    "data = load_dataset(dataset_path, split=\"train[:2048]\")\n",
    "assert isinstance(data, Dataset)\n",
    "\n",
    "# Tokenize the data (using a utils function) and shuffle it\n",
    "# tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=SEQ_LEN) # type: ignore\n",
    "# tokenized_data = tokenized_data.shuffle(42)\n",
    "\n",
    "# Get the tokens as a tensor\n",
    "all_tokens = torch.tensor(data[\"input_ids\"])\n",
    "# assert isinstance(all_tokens, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d066d1-1213-4145-bcd5-b08fdc77f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47265b78-9fb3-4472-91dd-88351335f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = api.sweep(f'{entity}/{project_name}/sweeps/{sweep_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53258a6e-f179-453b-baa6-a0f0dbf26931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download sae.pth from run lunar-sweep-8: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)\n",
      "Downloaded itifyaiz-files/desert-sweep-7_sae.pth from run desert-sweep-7\n",
      "Downloaded itifyaiz-files/worldly-sweep-6_sae.pth from run worldly-sweep-6\n",
      "Downloaded itifyaiz-files/flowing-sweep-5_sae.pth from run flowing-sweep-5\n",
      "Downloaded itifyaiz-files/laced-sweep-4_sae.pth from run laced-sweep-4\n",
      "Downloaded itifyaiz-files/brisk-sweep-3_sae.pth from run brisk-sweep-3\n",
      "Downloaded itifyaiz-files/vague-sweep-2_sae.pth from run vague-sweep-2\n",
      "Failed to download sae.pth from run stellar-sweep-1: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{sweep_id}-files'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for run in sweep.runs:\n",
    "    file_path = os.path.join(save_dir, f\"{run.name}_sae.pth\")\n",
    "\n",
    "    try:\n",
    "        file = run.file('sae.pth')\n",
    "        file.download(root=save_dir, replace=True)\n",
    "        downloaded_file_path = os.path.join(save_dir, 'sae.pth')\n",
    "        os.rename(downloaded_file_path, file_path)\n",
    "        print(f\"Downloaded {file_path} from run {run.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download sae.pth from run {run.name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49071dc2-3dd2-4dae-81aa-b0681df30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAEofSparseAutoencoder(sae: SparseAutoencoder) -> SAE:\n",
    "    d_hidden, d_in = sae.encoder.weight.shape\n",
    "    conf = SAEConfig(\n",
    "        architecture=\"standard\",\n",
    "        d_in=d_in,\n",
    "        d_sae=d_hidden,\n",
    "        activation_fn_str=\"relu\",\n",
    "        apply_b_dec_to_input=False,\n",
    "        finetuning_scaling_factor=False,\n",
    "        context_size=1024,  # TODO: what is this? does it matter?\n",
    "        model_name=\"gpt2\",\n",
    "        hook_name=hook_point,\n",
    "        hook_layer=6,\n",
    "        hook_head_index=None,\n",
    "        prepend_bos=False,\n",
    "        dataset_path=dataset_path,\n",
    "        dataset_trust_remote_code=False,\n",
    "        normalize_activations=False,\n",
    "        dtype=\"bfloat16\",\n",
    "        device=\"cpu\",\n",
    "        sae_lens_training_version=None,\n",
    "    )\n",
    "    result = SAE(conf)\n",
    "    result.W_enc.data = sae.encoder.weight.T\n",
    "    result.b_enc = sae.encoder.bias\n",
    "    result.W_dec.data = sae.decoder.weight.T\n",
    "    result.b_dec = sae.decoder.bias\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11b53db-09d3-4cf4-a6df-ce15a4c093e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = sweep.runs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0161550-e68f-4582-a7e2-32f077c0b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(save_dir, f\"{run.name}_sae.pth\")\n",
    "state_dict = torch.load(file_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65bcf110-950f-47ca-a73b-fc6482dc9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_ae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim, stddev_prior=run.config['stddev_prior'])\n",
    "sparse_ae.load_state_dict(state_dict)\n",
    "sae = SAEofSparseAutoencoder(sparse_ae)\n",
    "filename = os.path.join(save_dir, f\"{run.name}_vis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2af35ae2-8d5a-4c55-a2a1-6761b74acc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    n_batches_in_buffer=8,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1288ab5-3e2b-4db0-8867-eeafd6e11698",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = run_evals(\n",
    "    sae=sae,\n",
    "    activation_store=activations_store,\n",
    "    model=model,\n",
    "    eval_config=EvalConfig(\n",
    "        compute_kl=True,\n",
    "        compute_ce_loss=True,\n",
    "        compute_l2_norms=True,\n",
    "        compute_sparsity_metrics=True,\n",
    "        compute_variance_metrics=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41e4e6e-dd03-4dcb-83f5-9916d1bc4859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metrics/kl_div_with_sae': 0.4081176817417145,\n",
       " 'metrics/kl_div_with_ablation': 10.571954727172852,\n",
       " 'metrics/ce_loss_with_sae': 3.4435088634490967,\n",
       " 'metrics/ce_loss_without_sae': 3.0437071323394775,\n",
       " 'metrics/ce_loss_with_ablation': 13.629880905151367,\n",
       " 'metrics/kl_div_score': 0.9613961947176392,\n",
       " 'metrics/ce_loss_score': 0.962233594527192,\n",
       " 'metrics/l2_norm_in': 90.38226318359375,\n",
       " 'metrics/l2_norm_out': 85.30855560302734,\n",
       " 'metrics/l2_ratio': 0.942299485206604,\n",
       " 'metrics/l0': 592.2353515625,\n",
       " 'metrics/l1': 1976.57470703125,\n",
       " 'metrics/explained_variance': 0.8029875159263611,\n",
       " 'metrics/mse': 1013.3504638671875,\n",
       " 'metrics/total_tokens_evaluated': 81920}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d384215-073a-4891-8bb9-d0baf8d014d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vis_config_gpt = sae_vis_runner.SaeVisConfig(\n",
    "    hook_point=hook_point,\n",
    "    features=list(range(25)),\n",
    "    minibatch_size_features=2,\n",
    "    minibatch_size_tokens=1024,  # this is really prompt with the number of tokens determined by the sequence length\n",
    "    verbose=False,\n",
    "    device=\"cpu\",\n",
    "    cache_dir=Path(\n",
    "        \"demo_activations_cache\"\n",
    "    ),  # TODO: this will enable us to skip running the model for subsequent features.\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "runner = sae_vis_runner.SaeVisRunner(feature_vis_config_gpt)\n",
    "\n",
    "data = runner.run(\n",
    "    encoder=sae,\n",
    "    model=model,\n",
    "    tokens=all_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8ab51-ab3b-4d71-9524-6475c1c00f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "return\n",
    "for run in sweep.runs:\n",
    "    try:\n",
    "        file_path = os.path.join(save_dir, f\"{run.name}_sae.pth\")\n",
    "        state_dict = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "    except:\n",
    "        continue\n",
    "    sparse_ae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim, stddev_prior=run.config['stddev_prior'])\n",
    "    sparse_ae.load_state_dict(state_dict)\n",
    "    sae = SAEofSparseAutoencoder(sparse_ae)\n",
    "    filename = os.path.join(save_dir, f\"{run.name}_vis.html\")\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "\n",
    "    activations_store = ActivationsStore.from_sae(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        streaming=True,\n",
    "        store_batch_size_prompts=8,\n",
    "        n_batches_in_buffer=8,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    eval_metrics = run_evals(\n",
    "        sae=sae,\n",
    "        activation_store=activations_store,\n",
    "        model=model,\n",
    "        eval_config=EvalConfig(\n",
    "            compute_kl=True,\n",
    "            compute_ce_loss=True,\n",
    "            compute_l2_norms=True,\n",
    "            compute_sparsity_metrics=True,\n",
    "            compute_variance_metrics=True\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # CE Loss score should be high for residual stream SAEs\n",
    "    # ce loss without SAE should be fairly low < 3.5 suggesting the Model is being run correctly\n",
    "    # ce loss with SAE shouldn't be massively higher\n",
    "    print(eval_metrics)\n",
    "\n",
    "    feature_vis_config_gpt = sae_vis_runner.SaeVisConfig(\n",
    "        hook_point=hook_point,\n",
    "        features=list(range(25)),\n",
    "        minibatch_size_features=2,\n",
    "        minibatch_size_tokens=1024,  # this is really prompt with the number of tokens determined by the sequence length\n",
    "        verbose=False,\n",
    "        device=\"cpu\",\n",
    "        cache_dir=Path(\n",
    "            \"demo_activations_cache\"\n",
    "        ),  # TODO: this will enable us to skip running the model for subsequent features.\n",
    "        dtype=\"bfloat16\",\n",
    "    )\n",
    "    \n",
    "    runner = sae_vis_runner.SaeVisRunner(feature_vis_config_gpt)\n",
    "    \n",
    "    data = runner.run(\n",
    "        encoder=sae,\n",
    "        model=model,\n",
    "        tokens=all_tokens,\n",
    "    )\n",
    "\n",
    "    save_feature_centric_vis(sae_vis_data=data, filename=filename)\n",
    "\n",
    "    # sae_vis version:\n",
    "\n",
    "    # new_state_dict = {\n",
    "    #     \"W_enc\": state_dict[\"encoder.weight\"].T,\n",
    "    #     \"b_enc\": state_dict[\"encoder.bias\"],\n",
    "    #     \"W_dec\": state_dict[\"decoder.weight\"].T,\n",
    "    #     \"b_dec\": state_dict[\"decoder.bias\"],\n",
    "    # }\n",
    "\n",
    "    # d_hidden, d_in = state_dict[\"encoder.weight\"].shape\n",
    "    # cfg = AutoEncoderConfig(d_in=d_in, d_hidden=d_hidden)\n",
    "    # encoder = AutoEncoder(cfg)\n",
    "    # encoder.load_state_dict(new_state_dict)\n",
    "\n",
    "    # sae_vis_config = SaeVisConfig(\n",
    "    #     hook_point = \"blocks.6.hook_resid_post\",\n",
    "    #     features = range(64),\n",
    "    #     verbose = False,\n",
    "    # )\n",
    "    # \n",
    "    # sae_vis_data = SaeVisData.create(\n",
    "    #     encoder = encoder,\n",
    "    #     model = model,\n",
    "    #     tokens = all_tokens,\n",
    "    #     cfg = sae_vis_config,\n",
    "    # )\n",
    "    # \n",
    "    # sae_vis_data.save_feature_centric_vis(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c60c5f-4072-4774-8455-e3e36de5d817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
