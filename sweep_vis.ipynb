{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f5d479-c055-4f02-8510-10d73b1151ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from main import SparseAutoencoder, input_dim, hidden_dim, hook_point, model\n",
    "from sae_lens import SAE, SAEConfig\n",
    "from sae_dashboard import sae_vis_runner\n",
    "from sae_dashboard.feature_data_generator import FeatureDataGenerator\n",
    "from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "from sae_dashboard.sae_vis_runner import SaeVisConfig, SaeVisRunner\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from transformer_lens import utils\n",
    "from sae_lens import ActivationsStore, SAE, run_evals\n",
    "from sae_lens.evals import EvalConfig\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ae20e9-e504-4f5b-a719-f993b712bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = '1ds8ouf5'\n",
    "project_name = 'sae-expected-l0-sweep-norm'\n",
    "entity = 'PEAR-ML' \n",
    "hook_point = \"blocks.6.hook_resid_post\"\n",
    "dataset_path = \"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986e456b-18a6-4fe2-9e4e-b016b3dc8b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEQ_LEN = 128\n",
    "\n",
    "# Load in the data (it's a Dataset object)\n",
    "data = load_dataset(dataset_path, split=\"train[:2048]\")\n",
    "assert isinstance(data, Dataset)\n",
    "\n",
    "# Tokenize the data (using a utils function) and shuffle it\n",
    "# tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=SEQ_LEN) # type: ignore\n",
    "# tokenized_data = tokenized_data.shuffle(42)\n",
    "\n",
    "# Get the tokens as a tensor\n",
    "all_tokens = torch.tensor(data[\"input_ids\"])\n",
    "# assert isinstance(all_tokens, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d066d1-1213-4145-bcd5-b08fdc77f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47265b78-9fb3-4472-91dd-88351335f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = api.sweep(f'{entity}/{project_name}/sweeps/{sweep_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53258a6e-f179-453b-baa6-a0f0dbf26931",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{sweep_id}-files'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for run in sweep.runs:\n",
    "    file_path = os.path.join(save_dir, f\"{run.name}_sae.pth\")\n",
    "\n",
    "    try:\n",
    "        file = run.file('sae.pth')\n",
    "        file.download(root=save_dir, replace=True)\n",
    "        downloaded_file_path = os.path.join(save_dir, 'sae.pth')\n",
    "        os.rename(downloaded_file_path, file_path)\n",
    "        print(f\"Downloaded {file_path} from run {run.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download sae.pth from run {run.name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49071dc2-3dd2-4dae-81aa-b0681df30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAEofSparseAutoencoder(sae: SparseAutoencoder) -> SAE:\n",
    "    d_hidden, d_in = sae.encoder.weight.shape\n",
    "    conf = SAEConfig(\n",
    "        architecture=\"standard\",\n",
    "        d_in=d_in,\n",
    "        d_sae=d_hidden,\n",
    "        activation_fn_str=\"relu\",\n",
    "        apply_b_dec_to_input=False,\n",
    "        finetuning_scaling_factor=False,\n",
    "        context_size=1024,  # TODO: what is this? does it matter?\n",
    "        model_name=\"gpt2\",\n",
    "        hook_name=hook_point,\n",
    "        hook_layer=6,\n",
    "        hook_head_index=None,\n",
    "        prepend_bos=False,\n",
    "        dataset_path=dataset_path,\n",
    "        dataset_trust_remote_code=False,\n",
    "        normalize_activations=False,\n",
    "        dtype=\"bfloat16\",\n",
    "        device=\"cpu\",\n",
    "        sae_lens_training_version=None,\n",
    "    )\n",
    "    result = SAE(conf)\n",
    "    result.W_enc.data = sae.encoder.weight.T\n",
    "    result.b_enc = sae.encoder.bias\n",
    "    result.W_dec.data = sae.decoder.weight.T\n",
    "    result.b_dec = sae.decoder.bias\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8ab51-ab3b-4d71-9524-6475c1c00f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": [
       "[object Object]"
      ],
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devout-sweep-18 {'metrics/kl_div_with_sae': 0.09504439681768417, 'metrics/kl_div_with_ablation': 10.571810722351074, 'metrics/ce_loss_with_sae': 3.1370737552642822, 'metrics/ce_loss_without_sae': 3.0437216758728027, 'metrics/ce_loss_with_ablation': 13.629889488220215, 'metrics/kl_div_score': 0.9910096388108104, 'metrics/ce_loss_score': 0.9911816928423716, 'metrics/l2_norm_in': 90.38226318359375, 'metrics/l2_norm_out': 87.69502258300781, 'metrics/l2_ratio': 0.9695269465446472, 'metrics/l0': 1194.3394775390625, 'metrics/l1': 3965.449951171875, 'metrics/explained_variance': 0.9269226789474487, 'metrics/mse': 372.55401611328125, 'metrics/total_tokens_evaluated': 81920}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoic-sweep-17 {'metrics/kl_div_with_sae': 0.09539174288511276, 'metrics/kl_div_with_ablation': 10.571810722351074, 'metrics/ce_loss_with_sae': 3.136850118637085, 'metrics/ce_loss_without_sae': 3.0437216758728027, 'metrics/ce_loss_with_ablation': 13.629889488220215, 'metrics/kl_div_score': 0.9909767829380983, 'metrics/ce_loss_score': 0.9912028182043685, 'metrics/l2_norm_in': 90.38226318359375, 'metrics/l2_norm_out': 87.60328674316406, 'metrics/l2_ratio': 0.9684170484542847, 'metrics/l0': 1235.131591796875, 'metrics/l1': 4039.60107421875, 'metrics/explained_variance': 0.929223895072937, 'metrics/mse': 360.93505859375, 'metrics/total_tokens_evaluated': 81920}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lilac-sweep-16 {'metrics/kl_div_with_sae': 0.0849115327000618, 'metrics/kl_div_with_ablation': 10.571810722351074, 'metrics/ce_loss_with_sae': 3.12406325340271, 'metrics/ce_loss_without_sae': 3.0437216758728027, 'metrics/ce_loss_with_ablation': 13.629889488220215, 'metrics/kl_div_score': 0.9919681183356281, 'metrics/ce_loss_score': 0.9924107024417089, 'metrics/l2_norm_in': 90.38226318359375, 'metrics/l2_norm_out': 88.2266845703125, 'metrics/l2_ratio': 0.9754922389984131, 'metrics/l0': 1132.5970458984375, 'metrics/l1': 3689.04296875, 'metrics/explained_variance': 0.9344365000724792, 'metrics/mse': 333.1988525390625, 'metrics/total_tokens_evaluated': 81920}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revived-sweep-15 {'metrics/kl_div_with_sae': 0.09201941639184952, 'metrics/kl_div_with_ablation': 10.571810722351074, 'metrics/ce_loss_with_sae': 3.1274807453155518, 'metrics/ce_loss_without_sae': 3.0437216758728027, 'metrics/ce_loss_with_ablation': 13.629889488220215, 'metrics/kl_div_score': 0.9912957752641843, 'metrics/ce_loss_score': 0.9920878762808715, 'metrics/l2_norm_in': 90.38226318359375, 'metrics/l2_norm_out': 87.69331359863281, 'metrics/l2_ratio': 0.969398021697998, 'metrics/l0': 1129.5069580078125, 'metrics/l1': 3590.392578125, 'metrics/explained_variance': 0.9258410930633545, 'metrics/mse': 375.0377197265625, 'metrics/total_tokens_evaluated': 81920}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absurd-sweep-14 {'metrics/kl_div_with_sae': 0.08990734070539474, 'metrics/kl_div_with_ablation': 10.571810722351074, 'metrics/ce_loss_with_sae': 3.127901554107666, 'metrics/ce_loss_without_sae': 3.0437216758728027, 'metrics/ce_loss_with_ablation': 13.629889488220215, 'metrics/kl_div_score': 0.9914955589854336, 'metrics/ce_loss_score': 0.9920481254664527, 'metrics/l2_norm_in': 90.38226318359375, 'metrics/l2_norm_out': 87.85115051269531, 'metrics/l2_ratio': 0.9712569117546082, 'metrics/l0': 1238.2529296875, 'metrics/l1': 4042.947021484375, 'metrics/explained_variance': 0.9323223829269409, 'metrics/mse': 343.72265625, 'metrics/total_tokens_evaluated': 81920}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer-sweep-13 {'metrics/kl_div_with_sae': 0.08768874406814575, 'metrics/kl_div_with_ablation': 10.571810722351074, 'metrics/ce_loss_with_sae': 3.1241612434387207, 'metrics/ce_loss_without_sae': 3.0437216758728027, 'metrics/ce_loss_with_ablation': 13.629889488220215, 'metrics/kl_div_score': 0.9917054186485998, 'metrics/ce_loss_score': 0.9924014460197679, 'metrics/l2_norm_in': 90.38226318359375, 'metrics/l2_norm_out': 88.38250732421875, 'metrics/l2_ratio': 0.9772909879684448, 'metrics/l0': 1197.3472900390625, 'metrics/l1': 3912.5673828125, 'metrics/explained_variance': 0.9319438934326172, 'metrics/mse': 345.9559020996094, 'metrics/total_tokens_evaluated': 81920}\n"
     ]
    }
   ],
   "source": [
    "for run in sweep.runs:\n",
    "    try:\n",
    "        file_path = os.path.join(save_dir, f\"{run.name}_sae.pth\")\n",
    "        state_dict = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "    except:\n",
    "        continue\n",
    "    sparse_ae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim, stddev_prior=run.config['stddev_prior'])\n",
    "    sparse_ae.load_state_dict(state_dict)\n",
    "    sae = SAEofSparseAutoencoder(sparse_ae)\n",
    "    filename = os.path.join(save_dir, f\"{run.name}_vis.html\")\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "\n",
    "    activations_store = ActivationsStore.from_sae(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        streaming=True,\n",
    "        store_batch_size_prompts=8,\n",
    "        n_batches_in_buffer=8,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    eval_metrics = run_evals(\n",
    "        sae=sae,\n",
    "        activation_store=activations_store,\n",
    "        model=model,\n",
    "        eval_config=EvalConfig(\n",
    "            compute_kl=True,\n",
    "            compute_ce_loss=True,\n",
    "            compute_l2_norms=True,\n",
    "            compute_sparsity_metrics=True,\n",
    "            compute_variance_metrics=True\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # CE Loss score should be high for residual stream SAEs\n",
    "    # ce loss without SAE should be fairly low < 3.5 suggesting the Model is being run correctly\n",
    "    # ce loss with SAE shouldn't be massively higher\n",
    "    print(run.name, eval_metrics)\n",
    "\n",
    "    feature_vis_config_gpt = sae_vis_runner.SaeVisConfig(\n",
    "        hook_point=hook_point,\n",
    "        features=list(range(25)),\n",
    "        minibatch_size_features=2,\n",
    "        minibatch_size_tokens=64,  # this is really prompt with the number of tokens determined by the sequence length\n",
    "        verbose=False,\n",
    "        device=\"cpu\",\n",
    "        cache_dir=Path(\n",
    "            \"demo_activations_cache\"\n",
    "        ),  # TODO: this will enable us to skip running the model for subsequent features.\n",
    "        dtype=\"bfloat16\",\n",
    "    )\n",
    "    \n",
    "    runner = sae_vis_runner.SaeVisRunner(feature_vis_config_gpt)\n",
    "    \n",
    "    data = runner.run(\n",
    "        encoder=sae,\n",
    "        model=model,\n",
    "        tokens=all_tokens,\n",
    "    )\n",
    "\n",
    "    save_feature_centric_vis(sae_vis_data=data, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c60c5f-4072-4774-8455-e3e36de5d817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
